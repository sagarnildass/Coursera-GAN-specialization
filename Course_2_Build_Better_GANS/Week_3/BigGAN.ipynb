{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz7GMf9fruXG"
   },
   "source": [
    "# Components of BigGAN\n",
    "\n",
    "In this notebook, you'll learn about and implement the components of BigGAN, the first large-scale GAN architecture proposed in [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096) (Brock et al. 2019). BigGAN performs a conditional generation task, so unlike StyleGAN, it conditions on a certain class to generate results. BigGAN is based mainly on empirical results and shows extremely good results when trained on ImageNet and its 1000 classes.\n",
    "\n",
    "The authors propose a several changes that improve state-of-the-art Inception Score (IS) and Frechet Inception Distance (FID), including:\n",
    " - **Increasing batch size by a factor of 8**, which improves IS by 46% and improves FID by 35%, but also induces complete mode collapse in training.\n",
    " - **Increasing the number of convolutional channels by 1.5x**, which improves IS by 21% and FID by 23%.\n",
    " - **Using shared class-conditional embeddings $c$ in BatchNorm layers**, which reduces the number of parameters and increases IS by 2% and FID by 4%.\n",
    " - **Adding skip connections from latent noise $z$** by concatenating chunks of $z$ to $c$. This improves IS by 1% and FID by 5%.\n",
    "\n",
    "> ![BigGAN Architecture](BigGAN.png)\n",
    "*BigGAN Architecture Components, taken from Figure 15 in [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096) (Brock et al. 2019). (a) A typical architectural layout for BigGAN’s generator. See Appendix B for details. (b) A Residual Block (ResBlock up) in BigGAN’s generator. (c) A Residual Block (ResBlock down) in BigGAN’s discriminator.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vK-QqG_mefA"
   },
   "source": [
    "## The Truncation Trick and Orthogonal Regularization\n",
    "\n",
    "You should already be familiar with the truncation trick, which truncates the range of values of random noise $z$. Truncation to values close to 0 increases fidelity but decreases variety. Truncation to values further from 0 does the opposite.\n",
    "\n",
    "Truncation results in a different distribution of $z$ values from the one seen in training, which can cause saturation artifacts. The authors address this by making $G$ well-defined, or *smooth*, on the full distribution of $z$ values.\n",
    "\n",
    "To do this, they employ orthogonal regularization, first introduced in [Neural Photo Editing with Introspective Adversarial Networks](https://arxiv.org/abs/1609.07093) (Brock et al. 2017). The authors modify this regularization technique for BigGAN and formulate it as\n",
    "\n",
    "\\begin{align*}\n",
    "  R_\\beta(W) = \\beta\\big|\\big|W^\\top W \\odot (\\pmb{1} - I)\\big|\\big|^2_F,\n",
    "\\end{align*}\n",
    "where $\\pmb{1}$ denotes a matrix of 1's. This regularization term removes the diagonal terms from the regularization and aims to minimize the pairwise cosine similarity between filters without constraining their norm.\n",
    "\n",
    "> ![Truncation Trick](BigGAN-truncation-trick.png)\n",
    "*Generated images with different truncation thresholds, taken from Figure 2 in [Large Scale GAN Training for High Fidelity Natural Image Synthesis](https://arxiv.org/abs/1809.11096) (Brock et al. 2019). (a) The effects of increasing truncation. From left to right, the threshold is set to 2, 1, 0.5, 0.04. (b) Saturation artifacts from applying truncation to a poorly conditioned model.*\n",
    "\n",
    "Below is the implementation for orthogonal regularization. You can refer to the StyleGAN notebook for the truncation trick code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dPpIAAOjykrQ"
   },
   "outputs": [],
   "source": [
    "# Some setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def orthogonal_regularization(weight):\n",
    "    '''\n",
    "    Function for computing the orthogonal regularization term for a given weight matrix.\n",
    "    '''\n",
    "    weight = weight.flatten(1)\n",
    "    return torch.norm(\n",
    "        torch.dot(weight, weight) * (torch.ones_like(weight) - torch.eye(weight.shape[0]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAtedvsisf1j"
   },
   "source": [
    "## BigGAN Parts\n",
    "\n",
    "Before jumping into the full implementation, let's first take a look at some submodules that will be important in our BigGAN implementation later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-F0HvK8RMzI"
   },
   "source": [
    "### Class-conditional Batch Normalization\n",
    "\n",
    "Recall that batch norm aims to normalize activation statistics to a standard gaussian distribution (via an exponential moving average of minibatch mean and variances) but also applies trainable parameters, $\\gamma$ and $\\beta$, to invert this operation if the model sees fit:\n",
    "\n",
    "\\begin{align*}\n",
    "    y &= \\dfrac{x - \\hat{\\mu}}{\\hat{\\sigma} + \\epsilon} * \\gamma + \\beta.\n",
    "\\end{align*}\n",
    "\n",
    "BigGAN injects class-conditional information by parameterizing $\\gamma$ and $\\beta$ as linear transformations of the class embedding, $c$. Recall that BigGAN also concatenates $c$ with $z$ skip connections (denoted $[c, z]$), so\n",
    "\n",
    "\\begin{align*}\n",
    "    \\gamma &:= W_\\gamma^\\top[c, z] \\\\\n",
    "    \\beta &:= W_\\beta^\\top[c, z]\n",
    "\\end{align*}\n",
    "\n",
    "The idea is actually very similar to the adaptive instance normalization (AdaIN) module that you implemented in the StyleGAN notebook, so we've copied that code in comments below for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xCGq1YSz3ezh"
   },
   "outputs": [],
   "source": [
    "class ClassConditionalBatchNorm2d(nn.Module):\n",
    "    '''\n",
    "    ClassConditionalBatchNorm2d Class\n",
    "    Values:\n",
    "    in_channels: the dimension of the class embedding (c) + noise vector (z), a scalar\n",
    "    out_channels: the dimension of the activation tensor to be normalized, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bn = torch.nn.BatchNorm2d(out_channels)\n",
    "        self.class_scale_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\n",
    "        self.class_shift_transform = nn.utils.spectral_norm(nn.Linear(in_channels, out_channels, bias=False))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        normalized_image = self.bn(x)\n",
    "        class_scale = (1 + self.class_scale_transform(y))[:, :, None, None]\n",
    "        class_shift = self.class_shift_transform(y)[:, :, None, None]\n",
    "        transformed_image = class_scale * normalized_image + class_shift\n",
    "        return transformed_image\n",
    "\n",
    "# class AdaIN(nn.Module):\n",
    "#     '''\n",
    "#     AdaIN Class, extends/subclass of nn.Module\n",
    "#     Values:\n",
    "#       channels: the number of channels the image has, a scalar\n",
    "#       w_dim: the dimension of the intermediate tensor, w, a scalar \n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, channels, w_dim):\n",
    "#         super().__init__()\n",
    "#         self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "#         self.style_scale_transform = nn.Linear(w_dim, channels)\n",
    "#         self.style_shift_transform = nn.Linear(w_dim, channels)\n",
    "\n",
    "#     def forward(self, image, w):\n",
    "#         normalized_image = self.instance_norm(image)\n",
    "#         style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
    "#         style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
    "#         transformed_image = style_scale * normalized_image + style_shift\n",
    "#         return transformed_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5MrMUuvW5ac"
   },
   "source": [
    "### Self-Attention Block\n",
    "\n",
    "As you may already know, self-attention has been a successful technique in helping models learn arbitrary, long-term dependencies. [Self-Attention Generative Adversarial Networks](https://arxiv.org/abs/1805.08318) (Zhang et al. 2018) first introduced the self-attention mechanism into the GAN architecture. BigGAN augments its residual blocks with these attention blocks.\n",
    "\n",
    "**A Quick Primer on Self-Attention**\n",
    "\n",
    "Self-attention is just **scaled dot product attention**. Given a sequence $S$ (with images, $S$ is just the image flattened across its height and width), the model learns mappings to query ($Q$), key ($K$), and value ($V$) matrices:\n",
    "\n",
    "\\begin{align*}\n",
    "    Q &:= W_q^\\top S \\\\\n",
    "    K &:= W_k^\\top S \\\\\n",
    "    V &:= W_v^\\top S\n",
    "\\end{align*}\n",
    "\n",
    "where $W_q$, $W_k$, and $W_v$ are learned parameters. The subsequent self-attention mechanism is then computed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\dfrac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "\\end{align*}\n",
    "\n",
    "where $d_k$ is the dimensionality of the $Q, K$ matrices (SA-GAN and BigGAN both omit this term). Intuitively, you can think of the *query* matrix as containing the representations of each position with respect to itself and the *key* matrix as containing the representations of each position with respect to the others. How important two positions are to each other is measured by dot product as $QK^\\top$, hence **dot product attention**. A softmax is applied to convert these relative importances to a probability distribution over all positions.\n",
    "\n",
    "\n",
    "Intuitively, the *value* matrix provides the importance weighting of the attention at each position, hence **scaled dot product attention**. Relevant positions should be assigned larger weight and irrelevant ones should be assigned smaller weight.\n",
    "\n",
    "Don't worry if you don't understand this right away - it's a tough concept! For extra reading, you should check out [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Vaswani et al. 2017), which is the paper that first introduces this technique, and [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/), which breaks down and explains the self-attention mechanism clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "75rBFkY6XQGR"
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    '''\n",
    "    AttentionBlock Class\n",
    "    Values:\n",
    "    channels: number of channels in input\n",
    "    '''\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "\n",
    "        self.theta = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.phi = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 8, kernel_size=1, padding=0, bias=False))\n",
    "        self.g = nn.utils.spectral_norm(nn.Conv2d(channels, channels // 2, kernel_size=1, padding=0, bias=False))\n",
    "        self.o = nn.utils.spectral_norm(nn.Conv2d(channels // 2, channels, kernel_size=1, padding=0, bias=False))\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        spatial_size = x.shape[2] * x.shape[3]\n",
    "\n",
    "        # Apply convolutions to get query (theta), key (phi), and value (g) transforms\n",
    "        theta = self.theta(x)\n",
    "        phi = F.max_pool2d(self.phi(x), kernel_size=2)\n",
    "        g = F.max_pool2d(self.g(x), kernel_size=2)\n",
    "\n",
    "        # Reshape spatial size for self-attention\n",
    "        theta = theta.view(-1, self.channels // 8, spatial_size)\n",
    "        phi = phi.view(-1, self.channels // 8, spatial_size // 4)\n",
    "        g = g.view(-1, self.channels // 2, spatial_size // 4)\n",
    "\n",
    "        # Compute dot product attention with query (theta) and key (phi) matrices\n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), dim=-1)\n",
    "\n",
    "        # Compute scaled dot product attention with value (g) and attention (beta) matrices\n",
    "        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.channels // 2, x.shape[2], x.shape[3]))\n",
    "\n",
    "        # Apply gain and residual\n",
    "        return self.gamma * o + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChIw0phn3l2U"
   },
   "source": [
    "## BigGAN Generator\n",
    "\n",
    "Before implementing the generator in full, you first need to implement the generator residual block.\n",
    "\n",
    "### Generator Residual Block\n",
    "\n",
    "As with many state-of-the-art computer vision models, BigGAN employs skip connections in the form of residual blocks to map random noise to a fake image. You can think of BigGAN residual blocks as having 3 steps. Given input $x$ and class embedding $y$:\n",
    " 1. $h :=$ `bn-relu-upsample-conv`$(x, y)$\n",
    " 2. $h :=$ `bn-relu-conv`$(h, y)$\n",
    " 3. $x :=$ `upsample-conv`$(x)$,\n",
    "\n",
    "after which you can apply a residual connection and return $h + x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NZelofm5T-fy"
   },
   "outputs": [],
   "source": [
    "class GResidualBlock(nn.Module):\n",
    "    '''\n",
    "    GResidualBlock Class\n",
    "    Values:\n",
    "    c_dim: the dimension of conditional vector [c, z], a scalar\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, c_dim, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.bn1 = ClassConditionalBatchNorm2d(c_dim, in_channels)\n",
    "        self.bn2 = ClassConditionalBatchNorm2d(c_dim, out_channels)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.upsample_fn = nn.Upsample(scale_factor=2)     # upsample occurs in every gblock\n",
    "\n",
    "        self.mixin = (in_channels != out_channels)\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # h := upsample(x, y)\n",
    "        h = self.bn1(x, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.upsample_fn(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        # h := conv(h, y)\n",
    "        h = self.bn2(h, y)\n",
    "        h = self.activation(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        # x := upsample(x)\n",
    "        x = self.upsample_fn(x)\n",
    "        if self.mixin:\n",
    "            x = self.conv_mixin(x)\n",
    "\n",
    "        return h + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaXN6UeehAFm"
   },
   "source": [
    "You can now implement the BigGAN generator in full!! Below is an implementation of the base model (at 128x128 resolution) from the paper.\n",
    "\n",
    "> This implementation uses `nn.ModuleList` for convenience. If you're not familiar with this, you can think of it as simply a Pythonic list that registers your modules with the Pytorch backend. For more information, see the [torch.nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZoZ1WEbH3xLc"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "    z_dim: the dimension of random noise sampled, a scalar\n",
    "    shared_dim: the dimension of shared class embeddings, a scalar\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    bottom_width: the height/width of image before it gets upsampled, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    " \n",
    "    def __init__(self, base_channels=96, bottom_width=4, z_dim=120, shared_dim=128, n_classes=1000):\n",
    "        super().__init__()\n",
    " \n",
    "        n_chunks = 6    # 5 (generator blocks) + 1 (generator input)\n",
    "        self.z_chunk_size = z_dim // n_chunks\n",
    "        self.z_dim = z_dim\n",
    "        self.shared_dim = shared_dim\n",
    "        self.bottom_width = bottom_width\n",
    " \n",
    "        # No spectral normalization on embeddings, which authors observe to cripple the generator\n",
    "        self.shared_emb = nn.Embedding(n_classes, shared_dim)\n",
    " \n",
    "        self.proj_z = nn.Linear(self.z_chunk_size, 16 * base_channels * bottom_width ** 2)\n",
    " \n",
    "        # Can't use one big nn.Sequential since we are adding class+noise at each block\n",
    "        self.g_blocks = nn.ModuleList([\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 16 * base_channels, 16 * base_channels),\n",
    "                AttentionBlock(16 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 16 * base_channels, 8 * base_channels),\n",
    "                AttentionBlock(8 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 8 * base_channels, 4 * base_channels),\n",
    "                AttentionBlock(4 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 4 * base_channels, 2 * base_channels),\n",
    "                AttentionBlock(2 * base_channels),\n",
    "            ]),\n",
    "            nn.ModuleList([\n",
    "                GResidualBlock(shared_dim + self.z_chunk_size, 2 * base_channels, base_channels),\n",
    "                AttentionBlock(base_channels),\n",
    "            ]),\n",
    "        ])\n",
    "        self.proj_o = nn.Sequential(\n",
    "            nn.BatchNorm2d(base_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.utils.spectral_norm(nn.Conv2d(base_channels, 3, kernel_size=1, padding=0)),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    " \n",
    "    def forward(self, z, y):\n",
    "        '''\n",
    "        z: random noise with size self.z_dim\n",
    "        y: class embeddings with size self.shared_dim\n",
    "            = NOTE =\n",
    "            y should be class embeddings from self.shared_emb, not the raw class labels\n",
    "        '''\n",
    "        # Chunk z and concatenate to shared class embeddings\n",
    "        zs = torch.split(z, self.z_chunk_size, dim=1)\n",
    "        z = zs[0]\n",
    "        ys = [torch.cat([y, z], dim=1) for z in zs[1:]]\n",
    " \n",
    "        # Project noise and reshape to feed through generator blocks\n",
    "        h = self.proj_z(z)\n",
    "        h = h.view(h.size(0), -1, self.bottom_width, self.bottom_width)\n",
    " \n",
    "        # Feed through generator blocks\n",
    "        for idx, g_block in enumerate(self.g_blocks):\n",
    "            h = g_block[0](h, ys[idx])\n",
    "            h = g_block[1](h)\n",
    " \n",
    "        # Project to 3 RGB channels with tanh to map values to [-1, 1]\n",
    "        h = self.proj_o(h)\n",
    " \n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmWXtRgVfQMF"
   },
   "source": [
    "## BigGAN Discriminator\n",
    "\n",
    "Before implementing the discriminator in full, you need to implement a discriminator residual block, which is simpler than the generator's. Note that the last residual block does not apply downsampling.\n",
    " 1. $h :=$ `relu-conv-relu-downsample`$(x)$\n",
    " 2. $x :=$ `conv-downsample`$(x)$\n",
    "\n",
    "In the official BigGAN implementation, the architecture is slightly different for the first discriminator residual block, since it handles the raw image as input:\n",
    " 1. $h :=$ `conv-relu-downsample`$(x)$\n",
    " 2. $x :=$ `downsample-conv`$(x)$\n",
    "\n",
    "After these two steps, you can return the residual connection $h + x$. You might notice that there is no class information in these residual blocks. As you'll see later in the code, the authors inject class-conditional information after the final hidden layer (and before the output layer) via channel-wise dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JLm65xx1hoz-"
   },
   "outputs": [],
   "source": [
    "class DResidualBlock(nn.Module):\n",
    "    '''\n",
    "    DResidualBlock Class\n",
    "    Values:\n",
    "    in_channels: the number of channels in the input, a scalar\n",
    "    out_channels: the number of channels in the output, a scalar\n",
    "    downsample: whether to apply downsampling\n",
    "    use_preactivation: whether to apply an activation function before the first convolution\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, downsample=True, use_preactivation=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        self.conv2 = nn.utils.spectral_norm(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.use_preactivation = use_preactivation  # apply preactivation in all except first dblock\n",
    "\n",
    "        self.downsample = downsample    # downsample occurs in all except last dblock\n",
    "        if downsample:\n",
    "            self.downsample_fn = nn.AvgPool2d(2)\n",
    "        self.mixin = (in_channels != out_channels) or downsample\n",
    "        if self.mixin:\n",
    "            self.conv_mixin = nn.utils.spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0))\n",
    "\n",
    "    def _residual(self, x):\n",
    "        if self.use_preactivation:\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "        else:\n",
    "            if self.downsample:\n",
    "                x = self.downsample_fn(x)\n",
    "            if self.mixin:\n",
    "                x = self.conv_mixin(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply preactivation if applicable\n",
    "        if self.use_preactivation:\n",
    "            h = F.relu(x)\n",
    "        else:\n",
    "            h = x\n",
    "\n",
    "        h = self.conv1(h)\n",
    "        h = self.activation(h)\n",
    "        if self.downsample:\n",
    "            h = self.downsample_fn(h)\n",
    "\n",
    "        return h + self._residual(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4Re2GakokIU"
   },
   "source": [
    "Now implement the BigGAN discriminator in full!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "siICOXTaon2p"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "    base_channels: the number of base channels, a scalar\n",
    "    n_classes: the number of image classes, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=96, n_classes=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        # For adding class-conditional evidence\n",
    "        self.shared_emb = nn.utils.spectral_norm(nn.Embedding(n_classes, 16 * base_channels))\n",
    "\n",
    "        self.d_blocks = nn.Sequential(\n",
    "            DResidualBlock(3, base_channels, downsample=True, use_preactivation=False),\n",
    "            AttentionBlock(base_channels),\n",
    "\n",
    "            DResidualBlock(base_channels, 2 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(2 * base_channels),\n",
    "\n",
    "            DResidualBlock(2 * base_channels, 4 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(4 * base_channels),\n",
    "\n",
    "            DResidualBlock(4 * base_channels, 8 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(8 * base_channels),\n",
    "\n",
    "            DResidualBlock(8 * base_channels, 16 * base_channels, downsample=True, use_preactivation=True),\n",
    "            AttentionBlock(16 * base_channels),\n",
    "\n",
    "            DResidualBlock(16 * base_channels, 16 * base_channels, downsample=False, use_preactivation=True),\n",
    "            AttentionBlock(16 * base_channels),\n",
    "\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.proj_o = nn.utils.spectral_norm(nn.Linear(16 * base_channels, 1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        h = self.d_blocks(x)\n",
    "        h = torch.sum(h, dim=[2, 3])\n",
    "\n",
    "        # Class-unconditional output\n",
    "        uncond_out = self.proj_o(h)\n",
    "        if y is None:\n",
    "            return uncond_out\n",
    "\n",
    "        # Class-conditional output\n",
    "        cond_out = torch.sum(self.shared_emb(y) * h, dim=1, keepdim=True)\n",
    "        return uncond_out + cond_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj8aXxNLtGU_"
   },
   "source": [
    "## Setting Up BigGAN Training\n",
    "\n",
    "Now you're are ready to set up BigGAN for training! Unfortunately, this notebook will not provide actual training code due to the size of BigGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vdkfLu4wuHbh"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "# Initialize models\n",
    "base_channels = 96\n",
    "z_dim = 120\n",
    "n_classes = 5   # 5 classes is used instead of the original 1000, for efficiency\n",
    "shared_dim = 128\n",
    "generator = Generator(base_channels=base_channels, bottom_width=4, z_dim=z_dim, shared_dim=shared_dim, n_classes=n_classes).to(device)\n",
    "discriminator = Discriminator(base_channels=base_channels, n_classes=n_classes).to(device)\n",
    "\n",
    "# Initialize weights orthogonally\n",
    "for module in generator.modules():\n",
    "    if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "        nn.init.orthogonal_(module.weight)\n",
    "for module in discriminator.modules():\n",
    "    if (isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear) or isinstance(module, nn.Embedding)):\n",
    "        nn.init.orthogonal_(module.weight)\n",
    "\n",
    "# Initialize optimizers\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.0, 0.999), eps=1e-6)\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=4e-4, betas=(0.0, 0.999), eps=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHyU49y57PTb"
   },
   "source": [
    "Here is a sample forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E7YtKzgMxukz"
   },
   "outputs": [],
   "source": [
    "batch_size = n_classes\n",
    "\n",
    "z = torch.randn(batch_size, z_dim, device=device)                 # Generate random noise (z)\n",
    "y = torch.arange(start=0, end=n_classes, device=device).long()    # Generate a batch of labels (y), one for each class\n",
    "y_emb = generator.shared_emb(y)                                   # Retrieve class embeddings (y_emb) from generator\n",
    "\n",
    "x_gen = generator(z, y_emb)                                       # Generate fake images from z and y_emb\n",
    "score = discriminator(x_gen, y)                                   # Generate classification for fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pathology_train_july23_v2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 30\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, dr_grade\n\u001b[1;32m     24\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     25\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m)),\n\u001b[1;32m     26\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m     27\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m),\n\u001b[1;32m     28\u001b[0m             (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))])\n\u001b[0;32m---> 30\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDRDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpathology_train_july23_v2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./pathology_resized/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbsize\u001b[39m\u001b[38;5;124m'\u001b[39m], shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Training Loop\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m, in \u001b[0;36mDRDataset.__init__\u001b[0;34m(self, csv_file, root_dir, transform)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file, root_dir, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir \u001b[38;5;241m=\u001b[39m root_dir\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n",
      "File \u001b[0;32m~/anaconda3/envs/artelus/lib/python3.9/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/artelus/lib/python3.9/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/artelus/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/artelus/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/artelus/lib/python3.9/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pathology_train_july23_v2.csv'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "\n",
    "class DRDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index][\"new_path\"])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        dr_grade = torch.tensor(int(self.annotations.iloc[index]['dr_level']))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, dr_grade\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),\n",
    "            (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = DRDataset(csv_file='pathology_train_july23_v2.csv', root_dir='./pathology_resized/', transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=params['bsize'], shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "        # Update Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        discriminator.zero_grad()\n",
    "        \n",
    "        # Format batch\n",
    "        real_images, labels = data\n",
    "        real_images = real_images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = real_images.size(0)\n",
    "        \n",
    "        # Forward pass real batch through D\n",
    "        output_real = discriminator(real_images, labels).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = F.binary_cross_entropy_with_logits(output_real, torch.ones_like(output_real))\n",
    "        \n",
    "        # Generate fake image batch with G\n",
    "        noise = torch.randn(batch_size, z_dim, device=device)\n",
    "        fake_images = generator(noise, generator.shared_emb(labels))\n",
    "        # Classify all fake batch with D\n",
    "        output_fake = discriminator(fake_images.detach(), labels).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = F.binary_cross_entropy_with_logits(output_fake, torch.zeros_like(output_fake))\n",
    "        \n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Calculate the gradients for this batch, accumulated (summed) previously\n",
    "        errD.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Update Generator: maximize log(D(G(z)))\n",
    "        generator.zero_grad()\n",
    "        \n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = discriminator(fake_images, labels).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = F.binary_cross_entropy_with_logits(output, torch.ones_like(output))\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(train_loader),\n",
    "                     errD.item(), errG.item()))\n",
    "\n",
    "    # Checkpointing\n",
    "    if epoch % 10 == 0 or epoch == num_epochs-1:\n",
    "        # Save models\n",
    "        torch.save(generator.state_dict(), f'generator_epoch_{epoch}.pth')\n",
    "        torch.save(discriminator.state_dict(), f'discriminator_epoch_{epoch}.pth')\n",
    "        # Save sample generated images\n",
    "        with torch.no_grad():\n",
    "            fixed_noise = torch.randn(batch_size, z_dim, device=device)\n",
    "            fake_images = generator(fixed_noise, generator.shared_emb(labels))\n",
    "            vutils.save_image(fake_images, f'fake_images_epoch_{epoch}.png', normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-H2WoQDh-Fs"
   },
   "source": [
    "## BigGAN-deep\n",
    "\n",
    "Initially, the authors of the BigGAN paper didn't find much help in increasing the depth of the network. But they experimented further (research is always improving!) and added a few notes about an additional architecture, called BigGAN-deep. This modification of BigGAN is 4x deeper, sports a modified residual block architecture, and concatenates the entire $z$ vector to $c$ (as opposed to separate chunks at different resolutions).\n",
    "\n",
    "Typically on a difficult and complex task that you're unlikely to overfit, you expect better performance when a model has more parameters, because it has more room to learn. Surprisingly, BigGAN-deep has fewer parameters than its BigGAN counterpart. Architectural optimizations such as using depthwise separable convolutions and truncating/concatenating channels in skip connections (as opposed to using pointwise convolutions) decrease parameters without trading expressivity.\n",
    "\n",
    "For more details on the BigGAN-deep architecture, see Appendix B of the paper.\n",
    "\n",
    "And as for the implementation of the BigGAN-deep variant, well, that's left as an exercise for the reader. You're a smart cookie, you'll figure it out! Just keep in mind that with great power comes great responsibility ;)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C2W3: Components of BigGAN (Optional).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
